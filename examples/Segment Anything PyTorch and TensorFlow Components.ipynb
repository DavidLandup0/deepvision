{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36abfbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepvision-toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bae9fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepvision\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebecef7e",
   "metadata": {},
   "source": [
    "# TwoWayTransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbde4ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepvision.layers import TwoWayTransformerDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c822f5",
   "metadata": {},
   "source": [
    "### PyTorch TwoWayTransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c092d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1 = torch.randn([1, 256, 64, 64])\n",
    "inp2 = torch.randn([1, 256, 64, 64])\n",
    "inp3 = torch.randn([1, 7, 256])\n",
    "\n",
    "transformer = TwoWayTransformerDecoder(depth=8, \n",
    "                                       project_dim=256, \n",
    "                                       num_heads=8, \n",
    "                                       mlp_dim=256, \n",
    "                                       backend='pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "694d6415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__TwoWayTransformerDecoderPT(\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x __TwoWayAttentionBlockPT(\n",
       "      (self_attn): __DownscalingMultiheadAttentionPT(\n",
       "        (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (cross_attn_token_to_image): __DownscalingMultiheadAttentionPT(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): _MLPBlock(\n",
       "        (lin1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "      (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (cross_attn_image_to_token): __DownscalingMultiheadAttentionPT(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_attn_token_to_image): __DownscalingMultiheadAttentionPT(\n",
       "    (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "  )\n",
       "  (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e3899f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 256])\n",
      "torch.Size([1, 4096, 256])\n"
     ]
    }
   ],
   "source": [
    "outputs = transformer(inp1, inp2, inp3)\n",
    "for out in outputs:\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7237552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count 5414016\n"
     ]
    }
   ],
   "source": [
    "print('Param count', sum(p.numel() for p in transformer.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ef479",
   "metadata": {},
   "source": [
    "### TensorFlow TwoWayTransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66f97ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TwoWayTransformerDecoder(depth=8, \n",
    "                                       project_dim=256, \n",
    "                                       num_heads=8, \n",
    "                                       mlp_dim=256, \n",
    "                                       backend='tensorflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a98a3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 256)\n",
      "(1, 4096, 256)\n"
     ]
    }
   ],
   "source": [
    "inp1 = tf.random.uniform([1, 256, 64, 64])\n",
    "inp2 = tf.random.uniform([1, 256, 64, 64])\n",
    "inp3 = tf.random.uniform([1, 7, 256])\n",
    "\n",
    "outputs = transformer(inp1, inp2, inp3)\n",
    "for out in outputs:\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca5d15b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count 5414016\n"
     ]
    }
   ],
   "source": [
    "print('Param count', int(np.sum([K.count_params(p) for p in transformer.weights])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ba35e",
   "metadata": {},
   "source": [
    "# TwoWayAttentionBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "660c7f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepvision.layers import TwoWayAttentionBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57595647",
   "metadata": {},
   "source": [
    "### PyTorch TwoWayAttentionBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb44b20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__TwoWayAttentionBlockPT(\n",
       "  (self_attn): __DownscalingMultiheadAttentionPT(\n",
       "    (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (cross_attn_token_to_image): __DownscalingMultiheadAttentionPT(\n",
       "    (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "  )\n",
       "  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): _MLPBlock(\n",
       "    (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "    (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    (act): GELU(approximate='none')\n",
       "  )\n",
       "  (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (cross_attn_image_to_token): __DownscalingMultiheadAttentionPT(\n",
       "    (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_block = TwoWayAttentionBlock(project_dim=256, \n",
    "                                  num_heads=8, \n",
    "                                  mlp_dim=2048,  \n",
    "                                  backend='pytorch')\n",
    "attn_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e43c2ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 256])\n",
      "torch.Size([1, 4096, 256])\n"
     ]
    }
   ],
   "source": [
    "queries = torch.randn(1, 7, 256)\n",
    "keys = torch.randn(1, 4096, 256)\n",
    "key_pe = torch.randn(1, 4096, 256)\n",
    "query_pe = queries\n",
    "\n",
    "outputs = attn_block(queries=queries, keys=keys, query_pe=query_pe, key_pe=key_pe)\n",
    "for out in outputs:\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c0942c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count 1579520\n"
     ]
    }
   ],
   "source": [
    "print('Param count', sum(p.numel() for p in attn_block.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef8d33",
   "metadata": {},
   "source": [
    "### TensorFlow TwoWayAttentionBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a38f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_block = TwoWayAttentionBlock(project_dim=256, \n",
    "                                  num_heads=8, \n",
    "                                  mlp_dim=2048,  \n",
    "                                  backend='tensorflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52408379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 256)\n",
      "(1, 4096, 256)\n"
     ]
    }
   ],
   "source": [
    "queries = tf.random.uniform([1, 7, 256])\n",
    "keys = tf.random.uniform([1, 4096, 256])\n",
    "key_pe = tf.random.uniform([1, 4096, 256])\n",
    "query_pe = queries\n",
    "\n",
    "outputs = attn_block(queries=queries, keys=keys, query_pe=query_pe, key_pe=key_pe)\n",
    "for out in outputs:\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cd4348f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count 1579520\n"
     ]
    }
   ],
   "source": [
    "print('Param count', int(np.sum([K.count_params(p) for p in attn_block.weights])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df8a938",
   "metadata": {},
   "source": [
    "# DownscalingMultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "818c8152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepvision.layers import DownscalingMultiheadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81529a65",
   "metadata": {},
   "source": [
    "### PyTorch DownscalingMultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d5c2c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__DownscalingMultiheadAttentionPT(\n",
       "  (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_layer = DownscalingMultiheadAttention(256, 8, downsample_rate=1, backend=\"pytorch\")\n",
    "attn_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3999fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn(1, 7, 256)\n",
    "k = torch.randn(1, 4096, 256)\n",
    "v = torch.randn(1, 4096, 256)\n",
    "\n",
    "output = attn_layer(q=q, k=k, v=v)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96e25c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count 263168\n"
     ]
    }
   ],
   "source": [
    "print('Param count', sum(p.numel() for p in attn_layer.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b44c9f",
   "metadata": {},
   "source": [
    "### TensorFlow DownscalingMultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1480c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_layer = deepvision.layers.DownscalingMultiheadAttention(256, 8, downsample_rate=1, backend=\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e1a14c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 7, 256)\n"
     ]
    }
   ],
   "source": [
    "q = tf.random.uniform([1, 7, 256])\n",
    "k = tf.random.uniform([1, 4096, 256])\n",
    "v = tf.random.uniform([1, 4096, 256])\n",
    "\n",
    "output = attn_layer(q=q, k=k, v=v)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca74ec6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count 263168\n"
     ]
    }
   ],
   "source": [
    "print('Param count', int(np.sum([K.count_params(p) for p in attn_layer.weights])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e84639",
   "metadata": {},
   "source": [
    "# RelativePositionalTransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "624b852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepvision.layers import RelativePositionalTransformerEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61013b56",
   "metadata": {},
   "source": [
    "### PyTorch RelativePositionalTransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c87dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (3, 1024, 1024)\n",
    "patch_size = 14\n",
    "input_size = input_shape[1] // patch_size\n",
    "\n",
    "transformer = RelativePositionalTransformerEncoder(\n",
    "        project_dim=768,\n",
    "        num_heads=8,\n",
    "        mlp_dim=2048,\n",
    "        input_size=(input_size, input_size),\n",
    "        window_size=0,\n",
    "        backend='pytorch'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6807c062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__RelativePositionalTransformerEncoderPT(\n",
       "  (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): __RelativePositionalMultiheadAttentionPT(\n",
       "    (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "    (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): _MLPBlock(\n",
       "    (lin1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "    (lin2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "    (act): GELU(approximate='none')\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb4b16d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.randn(1, 64, 64, 768)\n",
    "outputs = transformer(inputs)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d70a02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count 5541824\n"
     ]
    }
   ],
   "source": [
    "print('Param count', sum(p.numel() for p in transformer.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e53979",
   "metadata": {},
   "source": [
    "### TensorFlow RelativePositionalTransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f366ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1024, 1024, 3)\n",
    "patch_size = 14\n",
    "input_size = input_shape[1] // patch_size\n",
    "\n",
    "transformer = RelativePositionalTransformerEncoder(\n",
    "        project_dim=768,\n",
    "        num_heads=8,\n",
    "        mlp_dim=2048,\n",
    "        input_size=(input_size, input_size),\n",
    "        window_size=0,\n",
    "        backend='tensorflow'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30d26610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nk_size=20.\\nq_size=10.\\n\\ntf.cast(tf.reshape(tf.range(q_size), [int(q_size), 1]), tf.float32) * tf.math.maximum(k_size / q_size, 1.0)\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "k_size=20.\n",
    "q_size=10.\n",
    "\n",
    "tf.cast(tf.reshape(tf.range(q_size), [int(q_size), 1]), tf.float32) * tf.math.maximum(k_size / q_size, 1.0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26bceb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 64, 64, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.random.uniform([1, 64, 64, 768])\n",
    "outputs = transformer(inputs)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "286e51b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count 5541824\n"
     ]
    }
   ],
   "source": [
    "print('Param count', int(np.sum([K.count_params(p) for p in transformer.weights])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3949ff2c",
   "metadata": {},
   "source": [
    "# RelativePositionalMultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2b42f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepvision.layers import RelativePositionalMultiheadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91af6957",
   "metadata": {},
   "source": [
    "### PyTorch RelativePositionalMultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2405002",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (3, 1024, 1024)\n",
    "patch_size = 14\n",
    "input_size = input_shape[1] // patch_size\n",
    "\n",
    "attn_layer = RelativePositionalMultiheadAttention(\n",
    "            project_dim=768,\n",
    "            num_heads=8,\n",
    "            qkv_bias=True,\n",
    "            use_rel_pos=True,\n",
    "            input_size=(input_size, input_size),\n",
    "            backend=\"pytorch\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "780f8a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__RelativePositionalMultiheadAttentionPT(\n",
       "  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c62dc42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.randn([1, 64, 64, 768])\n",
    "outputs = attn_layer(inputs)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78bface1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count 2390208\n"
     ]
    }
   ],
   "source": [
    "print('Param count', sum(p.numel() for p in attn_layer.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb955ee",
   "metadata": {},
   "source": [
    "### TensorFlow RelativePositionalMultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56225194",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (3, 1024, 1024)\n",
    "patch_size = 14\n",
    "input_size = input_shape[1] // patch_size\n",
    "\n",
    "attn_layer = RelativePositionalMultiheadAttention(\n",
    "            project_dim=768,\n",
    "            num_heads=8,\n",
    "            qkv_bias=True,\n",
    "            use_rel_pos=True,\n",
    "            input_size=(input_size, input_size),\n",
    "            backend=\"tensorflow\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82b44d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 64, 64, 768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.random.uniform([1, 64, 64, 768])\n",
    "outputs = attn_layer(inputs)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24f5d6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count 2390208\n"
     ]
    }
   ],
   "source": [
    "print('Param count', int(np.sum([K.count_params(p) for p in attn_layer.weights])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
